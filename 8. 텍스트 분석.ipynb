{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 분석"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텍스트 분석은 비정형 데이터인 텍스트를 분석하는 것입니다.\n",
    "- 머신러닝 알고리즘은 숫자형의 피처 기반 데이터만 입력받을 수 있기 때문에 비정형 데이터를 피처 형태로 추출하고 추출된 피처에 의미 있는 값을 부여하는 것이 매우 중요합니다.\n",
    "- 텍스트를 word 기반의 다수의 피처로 추출하고 단어 빈도수와 같은 숫자 값을 부여하면 텍스트는 벡터값으로 표현될 수 있는데, 이를 피처 벡터화(feature vectorization) 또는 피처 추출(feature extraction)이라 합니다.\n",
    "- 대표적인 피처 벡터화 변환 방법에는 BOW( bag of words )가 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텍스트 분석 수행 프로세스\n",
    "    1. 텍스트 사전 준비작업( 텍스트 전처리 )\n",
    "        - 클렌징, 대/소문자 변경, 특수문자 및 의미없는 단어 제거, 어근 추출등의 정규화 작업\n",
    "    2. 피처 벡터화/ 추출 \n",
    "        - 가공된 텍스트에서 피처를 추출하고 벡터 값 할당 -> BOW ( Count, TF-IDF )\n",
    "    3. ML 모델 수립 및 학습/ 예측/ 평가\n",
    "        - 피처 벡터화된 데이터 세트에 ML모델을 적용해 학습/ 예측 및 평가 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텍스트 분석 패키지\n",
    "    1. NLTK : 파이썬 대표적인 텍스트 분석 패키지, 수행 속도가 느려 실제 대량의 데이터 기반에서 활용 X\n",
    "    2. Gensim : 토픽 모델링 구현하는 기능으로 가장 많이 사용되는 패키지\n",
    "    3. SpaCy : 수행 성능이 뛰어나 많이 사용되는 패키지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 텍스트 전처리 ( 정규화 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 텍스트 정규화\n",
    "    1. 클렌징 : 불필요한 문자, 기호등을 사전에 제거하는 작업 ( HTML,XML등 ) \n",
    "    2. 텍스트 토큰화 : 문장을 분리하는 문장 토큰화, 문장에서 단어를 토큰으로 분리하는 단어 토큰화\n",
    "    3. 스톱 워드 제거 : 분석에 큰 의미가 없는 단어 제거\n",
    "    4. Stemming : 단어의 원형으로 변환 시 일반적인 단어를 적용하거나 단순화된 방법을 적용해 원래 단어에서 일부 철자가 훼손된 어근 단어를 추출하는 경향 ( Porter, Lancaster, Snowball Stemmer )\n",
    "    5. Lemmatiozation : 품사와 같은 문법적인 요소와 더 의미적인 부분을 감안해 정확한 철자로 된 어근 단어를 찾음 ( WordNetLemmatizer )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 문장 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', '| You feel it when you go to work, or go to church or pay your taxes.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize\n",
    "\n",
    "text_1 = 'The Matrix is everywhere its all around us, here even in this room. You can see it out your window or on your television. | You feel it when you go to work, or go to church or pay your taxes.'\n",
    "\n",
    "sentences = sent_tokenize(text = text_1)\n",
    "print(type(sentences), len(sentences))\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 단어 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 15\n",
      "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "text_2 = 'The Matrix is everywhere its all around us, here even in this room.'\n",
    "word = word_tokenize(text = text_2)\n",
    "print(type(word), len(word))\n",
    "print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'> 3\n",
      "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['|', 'You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import sent_tokenize, word_tokenize\n",
    "\n",
    "def tokenize(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    word = [ word_tokenize(sentence) for sentence in sentences ]\n",
    "    return word\n",
    "\n",
    "word_tokens = tokenize(text_1)\n",
    "print(type(word_tokens), len(word_tokens))\n",
    "print(word_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 스톱 워드 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "영어 stop word 개수: 179\n"
     ]
    }
   ],
   "source": [
    "print('영어 stop word 개수:', len(nltk.corpus.stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['|', 'feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "all_tokens = []\n",
    "for sentence in word_tokens:\n",
    "    filtered_words = []\n",
    "    for word in sentence:\n",
    "        word = word.lower()\n",
    "        \n",
    "        if word not in stopwords:\n",
    "            filtered_words.append(word)\n",
    "    all_tokens.append(filtered_words)\n",
    "print(all_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "work work work\n",
      "amus amus amus\n",
      "fant fanciest\n"
     ]
    }
   ],
   "source": [
    "from nltk import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "print(stemmer.stem('working'), stemmer.stem('works'), stemmer.stem('worked'))\n",
    "print(stemmer.stem('amusing'), stemmer.stem('amuses'), stemmer.stem('amused'))\n",
    "print(stemmer.stem('fancier'), stemmer.stem('fanciest'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amuse amuse amuse\n",
      "fancy fancy\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "lemma = WordNetLemmatizer()\n",
    "print(lemma.lemmatize('amusing','v'), lemma.lemmatize('amuses','v'),lemma.lemmatize('amused','v'))\n",
    "print(lemma.lemmatize('fancier','a'), lemma.lemmatize('fanciest','a'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 'v' : 동사\n",
    "- 'a' : 형용사"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BOW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Bag of Words는 문서가 가지는 모든 단어를 문맥이나 순서를 무시하고 일괄적으로 단어에 대해 빈도 값을 부여해 피처 값을 추출하는 모델입니다.\n",
    "- 장점\n",
    "    - 쉽고 빠른 구축 가능\n",
    "- 단점\n",
    "    - 문맥 의미 반영 부족 : 단어의 순서를 고려하지 않기 대문에 문장 내에서 문맥적인 의미가 무시\n",
    "    - 희소 행렬 문제 : bow로 피처 벡터화를 수행하면 희소 행렬 형태의 데이터 세트가 만들어지기 쉬움. 희소 행렬은 ML알고리즘의 수행 시간과 예측 성능을 떨어트림"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BOW 피처 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ML 알고리즘은 숫자형 피처를 데이터로 입력받아 동작하기 대문에 텍스트와 같은 데이터는 숫자형 값인 벡터 값으로 변환해야 하는데, 이러한 변환을 피처 벡터화라고 합니다.\n",
    "- 텍스트를 단어로 추출해 피처로 할당하고, 각 단어의 발생 빈도와 같은 값을 피처에 값으로 부여해 피처의 발생 빈도 값으로 구성된 벡터로 만드는 기법입니다.\n",
    "- BOW 모델에서 피처 벡터화를 수행한다는 것은 모든 문서에서 모든 단어를 칼럼 형태로 나열하고 각 문서에서 해당 단어의 횟수나 정규화된 빈도를 값으로 부여하는 데이터 세트 모델로 변경하는 것입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- BOW 종류 \n",
    "    - 카운트 기반의 벡터화\n",
    "    - TF-IDF( term frequency - inverse document frequency )기반의 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 카운트 기반의 벡터화 \n",
    "    - 문서에서 해당 단어가 나타는 횟수, count를 부여하는 경우를 카운트 벡터화라고 하며, 카운트 값이 높을수록 중요한 단어로 인식합니다.\n",
    "    - 하지만 카운트만 부여할 경우 문서의 특징을 나타내기보다는 언어의 특성상 문장에서 자주 사용될 수밖에 없는 단어까지 높은 값을 부여합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- TF-IDF 기반의 벡터화\n",
    "    - 카운트 기반의 벡터화의 단점을 보완하기 위한 것으로, 개별 문서에서 자주 나타나는 단어에 높은 가중치를 주되, 모든 문서에서 전반적으로 자주 나타나는 단어에 대해서 페널티를 주는 방식으로 값을 부여합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 사이킷런의 Count 및 TF-IDF 벡터화 구현 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사이킷런의 Count 및 TF-IDF 벡터화 종류\n",
    "    - CountVectorizer : 카운트 기반의 벡터화를 구현한 클래스로 소문자 일괄 변환, 토큰화, 스톱워드 필터링등의 텍스트 전처리도 함께 수행\n",
    "        - CountVectorizer 입력 파라미터 \n",
    "            - max_df : 문서에서 너무 높은 빈도수를 가지는 단어 피처를 제외하기 위한 파라미터\n",
    "            - min_df : 문서에서 너무 낮은 빈도수를 가지는 단어 피처를 제외하기 위한 파라미터\n",
    "            - max_features : 피처의 개수가 높은 빈도를 가지는 단어 순으로 피처 추출해 정수로 값 지정\n",
    "            - stop_words : 'english'로 지정하면 영어의 스톱 워드로 지정된 단어는 추출에서 제외\n",
    "            - n_gram_range : bow 모델의 단어 순서를 보강하기 위해 범위 설정, 튜플 형태로 최소,최댓값 지정\n",
    "            - analyzer : 피처 추출을 수행한 단위 지정, default는 'word'\n",
    "            - token_pattern :  토큰화를 수행하는 정규 표현식 패턴을 지정, defalut는 '|b|w|w+|b'로, 공백 또는 개행 문자 등으로 구분된 단어 분리자(|b) 사이의 2문자 이상의 단어를 토큰으로 분리\n",
    "            - tokenizer : 토큰화를 별도의 사용자 함수로 이용시 적용. CountTokenizer 클래스에서 어근 변환 시 이를 수행하는 별도의 함수를 tokenizer 파라미터에 적용\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CountVectorizer를 이용한 피처 벡터화 과정\n",
    "    1. 사전 데이터 가공 : 모든 문자를 소문자로 변환하는 등의 사전 작업 수행.\n",
    "    2. 토큰화 : default는 단어 기준( analyzer = True )이며, n_gram_range를 반영하여 토큰화 수행\n",
    "    3. 텍스트 정규화 : stop words 필터링만 수행, Stemmer, Lemmatize는 외부 함수를 만들거나 패키지로 Text Normalization 수행 필요\n",
    "    4. 피처 벡터화 : 파라미터를 반영해 token된 단어들을 feature extraction 후 vectorization 적용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BOW 벡터화를 위한 희소 행렬\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사이킷런의 CountVectorizer/TfidfVectorizer를 이용해 텍스트를 피처 단위로 벡터화한다면 대규모 행렬의 대부분의 값을 0이 차지하는 행렬을 가리키는 희소 행렬이 대부분입니다.\n",
    "- 불필요한 값이 메모리 공간에 할당되어 메모리 공간이 많이 필요하며, 연산 시에도 데이터 액세스를 위한 시간이 많이 소모되기 때문에 이러한 희소 행렬이 적은 메모리 공간을 차지할 수 있도록 변환할 수 있는 방법으로 COO, CSR 존재합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- COO \n",
    "    - 0이 아닌 데이터만 별도의 데이터 배열에 저장하고, 그 데이터가 가리키는 행과 열의 위치를 별도의 배열로 저장\n",
    "    - 희소 행렬 변환을 위해서 사이파이( Scipy )의 sparse를 이용해 COO형식으로 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sample = np.array([[3, 0, 1],[0, 2, 0]])\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3, 0, 1],\n",
       "       [0, 2, 0]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "data = np.array([3,1,2])\n",
    "\n",
    "row = np.array([0, 0, 1])\n",
    "col = np.array([0, 2, 1])\n",
    "\n",
    "sparse_coo = sparse.coo_matrix((data, (row, col)))\n",
    "\n",
    "sparse_coo.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CSR 형식\n",
    "    - COO 형식이 행과 열의 위치를 나타내기 위해서 반복적인 위치 데이터를 사용해야 하는 문제점을 해결한 방식"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n",
      "CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력\n",
      "[[0 0 1 0 0 5]\n",
      " [1 4 0 3 2 5]\n",
      " [0 6 0 3 0 0]\n",
      " [2 0 0 0 0 0]\n",
      " [0 0 0 7 0 8]\n",
      " [1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "\n",
    "sample_2 = np.array([[0,0,1,0,0,5],\n",
    "             [1,4,0,3,2,5],\n",
    "             [0,6,0,3,0,0],\n",
    "             [2,0,0,0,0,0],\n",
    "             [0,0,0,7,0,8],\n",
    "             [1,0,0,0,0,0]])\n",
    "\n",
    "data_2 = np.array([1, 5, 1, 4, 3, 2, 5, 6, 3, 2, 7, 8, 1])\n",
    "\n",
    "row_2 = np.array([0, 0, 1, 1, 1, 1, 1, 2, 2, 3, 4, 4, 5])\n",
    "col_2 = np.array([2, 5, 0, 1, 3, 4, 5, 1, 3, 0, 3, 5, 0])\n",
    "\n",
    "sparse_coo = sparse.coo_matrix((data_2,(row_2,col_2)))\n",
    "\n",
    "row_pos = np.array([0, 2, 7, 9, 10 ,12, 13])\n",
    "\n",
    "sparse_csr = sparse.csr_matrix((data_2,col_2, row_pos))\n",
    "\n",
    "print('COO 변환된 데이터가 제대로 되었는지 다시 Dense로 출력')\n",
    "print(sparse_coo.toarray())\n",
    "print('CSR 변환된 데이터가 제대로 되었는지 다시 Dense로 출력')\n",
    "print(sparse_csr.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
